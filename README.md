The goal of this project was to adapt Meta's LLaMA-2 large language model for the healthcare domain by fine-tuning it on a medical question-answering dataset. The objective was to enable the model to understand patient queries and generate medically relevant responses that resemble real-world doctor-patient interactions.

Using the MedDialog (English) dataset, which contains thousands of anonymized medical consultations, the model was trained using Low-Rank Adaptation (LoRA) and 4-bit quantization for memory-efficient fine-tuning. The Hugging Face transformers, datasets, and peft libraries were used for training, tokenization, and parameter-efficient optimization.
